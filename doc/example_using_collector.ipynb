{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the EWXPWSDB Collector Class\n",
    "\n",
    "# <span style=\"color:red\">clear all output before saving: db output contains passwords! </span>\n",
    "\n",
    "this walks through process of\n",
    "\n",
    "- creating a temporary DB\n",
    "- using the collector class for existing station records to\n",
    "    - get past data \n",
    "    - pull data for short period, e.g. from a scheduler\n",
    "    - get all recent data, aka catch up data from last record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants/Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "station_file = '../data/test_stations.tsv'\n",
    "station_type = 'DAVIS'\n",
    "station_code = 'EWXDAVIS01' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ewxpwsdb.db.database import Session, init_db, get_db_url, get_engine\n",
    "from ewxpwsdb.db.models import WeatherStation, Reading, StationType, APIResponse\n",
    "from ewxpwsdb.db.importdata import import_station_file\n",
    "from ewxpwsdb.collector import Collector\n",
    "from ewxpwsdb.time_intervals import UTCInterval\n",
    "from sqlmodel import select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create engine temp database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ewxpwsdb.db.database import temp_pg_engine\n",
    "engine = temp_pg_engine(host='localhost')\n",
    "\n",
    "temp_db_url = engine.url\n",
    "print(temp_db_url.database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_db(engine,station_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_station(station_type, station_code = None, engine=engine):\n",
    "    \"\"\" global engine is default\"\"\"\n",
    "\n",
    "    if station_code:\n",
    "        statement = select(WeatherStation).where(WeatherStation.station_code == station_code)\n",
    "    else:\n",
    "        statement = select(WeatherStation).where(WeatherStation.station_type == station_type)\n",
    "\n",
    "    with Session(engine) as session:\n",
    "        results = session.exec(statement)\n",
    "        weather_station = results.first()\n",
    "\n",
    "    return weather_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = get_one_station(station_type, station_code, engine)\n",
    "print(station.station_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = Collector(station, engine)\n",
    "collector.weather_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector._session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from ewxpwsdb.time_intervals import UTCInterval\n",
    "duration_min = 70\n",
    "viable_interval = UTCInterval.previous_interval(delta_mins=duration_min)\n",
    "yesterday = UTCInterval(start=viable_interval.start - timedelta(days = 1), \n",
    "                           end = viable_interval.end - timedelta(days = 1)\n",
    "                           )\n",
    "yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from ewxpwsdb.time_intervals import previous_fourteen_minute_interval\n",
    "\n",
    "interval = previous_fourteen_minute_interval()\n",
    "\n",
    "interval.start = interval.start - timedelta(hours = 1)\n",
    "interval.end = interval.end - timedelta(hours = 0.5)\n",
    "interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_interval = UTCInterval.one_day_interval()  # this defaults to getting the time range from midnight to now\n",
    "two_day_interval = UTCInterval(start = (today_interval.start - timedelta(days = 1)), end = today_interval.end)\n",
    "two_day_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "today_utc = datetime.now(timezone.utc).date()\n",
    "\n",
    "collector.request_and_store_weather_data_utc(UTCInterval.one_day_interval(d = today_utc- timedelta(days = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector.get_readings(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the method to see if it's working\n",
    "somerex = collector.request_and_store_weather_data_utc(interval)\n",
    "somerex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are api response ids being saved in the object?\n",
    "\n",
    "collector.current_api_response_record_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to re-fill this sqlalchemy record cache, just ask for some piece of the data\n",
    "print(collector.current_api_response.id)\n",
    "# now the object cache is refilled and should be present\n",
    "collector.current_api_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the transformed readings, if any\n",
    "collector.current_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, demonstrate that the readings were stored in the database by checking the ID field\n",
    "collector.current_readings[0].id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/Demo getting a full day of readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viable_interval = UTCInterval.previous_fifteen_minutes()\n",
    "collector.request_and_store_weather_data_utc(viable_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesterday = UTCInterval(start=viable_interval.start - timedelta(days = 1),\n",
    "                             end = viable_interval.end - timedelta(days = 1)\n",
    "                               )\n",
    "print(yesterday)\n",
    "collector._session.rollback()\n",
    "\n",
    "response_ids = collector.request_and_store_weather_data_utc(yesterday)\n",
    "print(\"response ids:\")\n",
    "print(response_ids)\n",
    "print(\"reading ids:\")\n",
    "print(collector.current_reading_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## there _should_ be readings from the same interval in there now     \n",
    "\n",
    "readings = collector.get_readings_by_date(yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/demo of restricting data inserts\n",
    "\n",
    "This used to throw an exception when saving readings with the same timestamp and station. \n",
    "however, even though this is a unique constraint on these columns, the collector code checks for that, and simply \n",
    "updates the record.  This is known as an 'upsert' but it doesn't using the Postgresql + SQLAlchemy upsert but rather some custom code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "\n",
    "try:\n",
    "    something = collector.save_readings_from_responses(api_responses = collector.current_api_response)\n",
    "except IntegrityError as e:\n",
    "    collector._session.rollback()\n",
    "    print(\"integrity error prevented duplicate records from being inserted\")\n",
    "\n",
    "# what happens to the current readings? \n",
    "collector.current_readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the rollback worked\n",
    "\n",
    "try:\n",
    "    something = collector.save_readings_from_responses(api_responses = collector.current_api_response)\n",
    "except IntegrityError as e:\n",
    "    collector._session.rollback()\n",
    "    print(\"didn't get the rollback error\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do we still have radings after a rollback?\n",
    "readings = collector.get_readings(n=5)\n",
    "readings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "remove test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ewxpwsdb.db.database import drop_temp_pg_engine, list_pg_databases\n",
    "from sqlalchemy.orm import close_all_sessions\n",
    "\n",
    "if collector:\n",
    "    collector._session.close()\n",
    "    collector._engine.dispose()\n",
    "\n",
    "close_all_sessions()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"attempting to drop db {engine.url.database}\")\n",
    "result = drop_temp_pg_engine(engine)\n",
    "print(result)\n",
    "engine.dispose()\n",
    "list_pg_databases(host='localhost')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ewxpwsdb-Ylvp0c_2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
